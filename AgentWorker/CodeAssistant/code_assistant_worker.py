"""
ä»£ç åŠ©æ‰‹AgentWorker

ä¸“é—¨ç”¨äºä»£ç ç”Ÿæˆã€ä»£ç ä¼˜åŒ–ã€ä»£ç è§£é‡Šã€ä»£ç å®¡æŸ¥ç­‰ä»»åŠ¡çš„Agentå·¥ä½œæµ
"""

import asyncio
import sys
from pathlib import Path
from typing import Dict, Any, Optional, List
import re

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.append(str(PROJECT_ROOT / "src"))

from core.output_control import OutputController
from core.config.llm_manager import LLMManager


class CodeAssistantWorker:
    """ä»£ç åŠ©æ‰‹å·¥ä½œå™¨"""
    
    def __init__(self, llm_model: str = "gpt-4o"):
        """
        åˆå§‹åŒ–ä»£ç åŠ©æ‰‹å·¥ä½œå™¨
        
        Args:
            llm_model: ä½¿ç”¨çš„LLMæ¨¡å‹ï¼Œä»£ç ç”Ÿæˆæ¨èä½¿ç”¨å¼ºæ¨¡å‹
        """
        self.llm_model = llm_model
        self.llm_manager = LLMManager()
        
        # ä»æœ¬åœ°é…ç½®åŠ è½½è¾“å‡ºæ§åˆ¶å™¨
        config_path = Path(__file__).parent / "configs" / "output_control.yaml"
        self.output_controller = OutputController.from_config_file(
            str(config_path),
            llm_client=self.llm_manager.get_client()
        )
        
        print(f"âœ… ä»£ç åŠ©æ‰‹å·¥ä½œå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {llm_model}")
    
    async def generate_function(self, function_description: str, language: str = "python", 
                               include_tests: bool = True) -> Dict[str, Any]:
        """
        ç”Ÿæˆå‡½æ•°ä»£ç 
        
        Args:
            function_description: å‡½æ•°åŠŸèƒ½æè¿°
            language: ç¼–ç¨‹è¯­è¨€
            include_tests: æ˜¯å¦åŒ…å«æµ‹è¯•ä»£ç 
            
        Returns:
            åŒ…å«ç”Ÿæˆçš„å‡½æ•°ä»£ç å’Œè´¨é‡ä¿¡æ¯çš„å­—å…¸
        """
        test_requirement = "å¹¶æä¾›ç›¸åº”çš„å•å…ƒæµ‹è¯•ä»£ç " if include_tests else ""
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹æè¿°ç”Ÿæˆ{language}å‡½æ•°{test_requirement}ï¼š

åŠŸèƒ½æè¿°ï¼š{function_description}
ç¼–ç¨‹è¯­è¨€ï¼š{language}

è¦æ±‚ï¼š
1. å‡½æ•°å®šä¹‰æ¸…æ™°ï¼Œå‚æ•°å’Œè¿”å›å€¼æœ‰ç±»å‹æ³¨è§£ï¼ˆå¦‚æœè¯­è¨€æ”¯æŒï¼‰
2. åŒ…å«è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²è¯´æ˜å‡½æ•°ç”¨é€”ã€å‚æ•°ã€è¿”å›å€¼
3. ä»£ç é€»è¾‘æ¸…æ™°ï¼Œéµå¾ªæœ€ä½³å®è·µ
4. åŒ…å«é€‚å½“çš„é”™è¯¯å¤„ç†
5. ä½¿ç”¨ä»£ç å—æ ¼å¼ï¼ŒåŒ…å«è¯­æ³•é«˜äº®
{f'6. æä¾›å®Œæ•´çš„å•å…ƒæµ‹è¯•ç”¨ä¾‹' if include_tests else ''}

è¯·ç¡®ä¿ä»£ç å®‰å…¨ã€é«˜æ•ˆï¼Œé¿å…ä½¿ç”¨å±é™©å‡½æ•°ã€‚"""

        # è°ƒç”¨LLMç”Ÿæˆä»£ç 
        llm_client = self.llm_manager.get_client()
        initial_output = await llm_client.generate(prompt, model=self.llm_model)
        
        # åº”ç”¨è¾“å‡ºæ§åˆ¶
        control_result = await self.output_controller.control_output(
            original_prompt=prompt,
            agent_output=initial_output,
            output_type="function"
        )
        
        return {
            "success": control_result.success,
            "content": control_result.final_output,
            "function_description": function_description,
            "language": language,
            "include_tests": include_tests,
            "quality_score": self._calculate_code_quality(control_result),
            "security_score": self._assess_security(control_result),
            "processing_time": control_result.processing_time,
            "processing_info": self._get_processing_info(control_result)
        }
    
    async def generate_class(self, class_description: str, language: str = "python",
                            include_examples: bool = True) -> Dict[str, Any]:
        """
        ç”Ÿæˆç±»ä»£ç 
        
        Args:
            class_description: ç±»åŠŸèƒ½æè¿°
            language: ç¼–ç¨‹è¯­è¨€
            include_examples: æ˜¯å¦åŒ…å«ä½¿ç”¨ç¤ºä¾‹
            
        Returns:
            åŒ…å«ç”Ÿæˆçš„ç±»ä»£ç å’Œè´¨é‡ä¿¡æ¯çš„å­—å…¸
        """
        example_requirement = "å¹¶æä¾›ä½¿ç”¨ç¤ºä¾‹" if include_examples else ""
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹æè¿°ç”Ÿæˆ{language}ç±»{example_requirement}ï¼š

ç±»æè¿°ï¼š{class_description}
ç¼–ç¨‹è¯­è¨€ï¼š{language}

è¦æ±‚ï¼š
1. ç±»è®¾è®¡åˆç†ï¼Œéµå¾ªå•ä¸€èŒè´£åŸåˆ™
2. åŒ…å«å¿…è¦çš„æ„é€ å‡½æ•°ã€å±æ€§å’Œæ–¹æ³•
3. æ¯ä¸ªæ–¹æ³•éƒ½æœ‰æ¸…æ™°çš„æ–‡æ¡£è¯´æ˜
4. ä½¿ç”¨é€‚å½“çš„è®¿é—®ä¿®é¥°ç¬¦ï¼ˆå¦‚æœè¯­è¨€æ”¯æŒï¼‰
5. åŒ…å«é”™è¯¯å¤„ç†å’Œå‚æ•°éªŒè¯
6. éµå¾ªå‘½åè§„èŒƒå’Œä»£ç é£æ ¼
{f'7. æä¾›è¯¦ç»†çš„ä½¿ç”¨ç¤ºä¾‹ä»£ç ' if include_examples else ''}

è¯·ç¡®ä¿ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•ã€‚"""

        # è°ƒç”¨LLMç”Ÿæˆä»£ç 
        llm_client = self.llm_manager.get_client()
        initial_output = await llm_client.generate(prompt, model=self.llm_model)
        
        # åº”ç”¨è¾“å‡ºæ§åˆ¶
        control_result = await self.output_controller.control_output(
            original_prompt=prompt,
            agent_output=initial_output,
            output_type="class"
        )
        
        return {
            "success": control_result.success,
            "content": control_result.final_output,
            "class_description": class_description,
            "language": language,
            "include_examples": include_examples,
            "quality_score": self._calculate_code_quality(control_result),
            "design_quality": self._assess_design_quality(control_result)
        }
    
    async def optimize_code(self, code: str, optimization_goals: List[str]) -> Dict[str, Any]:
        """
        ä¼˜åŒ–ä»£ç 
        
        Args:
            code: å¾…ä¼˜åŒ–çš„ä»£ç 
            optimization_goals: ä¼˜åŒ–ç›®æ ‡åˆ—è¡¨ï¼ˆæ€§èƒ½ã€å¯è¯»æ€§ã€å®‰å…¨æ€§ç­‰ï¼‰
            
        Returns:
            åŒ…å«ä¼˜åŒ–åä»£ç å’Œæ”¹è¿›è¯´æ˜çš„å­—å…¸
        """
        goals_text = "ã€".join(optimization_goals)
        
        prompt = f"""è¯·å¯¹ä»¥ä¸‹ä»£ç è¿›è¡Œä¼˜åŒ–ï¼Œä¼˜åŒ–é‡ç‚¹ï¼š{goals_text}

åŸå§‹ä»£ç ï¼š
```
{code}
```

ä¼˜åŒ–è¦æ±‚ï¼š
1. ä¿æŒåŸæœ‰åŠŸèƒ½ä¸å˜
2. é’ˆå¯¹æŒ‡å®šç›®æ ‡è¿›è¡Œä¼˜åŒ–ï¼š{goals_text}
3. æä¾›ä¼˜åŒ–åçš„å®Œæ•´ä»£ç 
4. è¯¦ç»†è¯´æ˜æ¯ä¸ªä¼˜åŒ–ç‚¹å’Œæ”¹è¿›ç†ç”±
5. æŒ‡å‡ºæ½œåœ¨çš„é—®é¢˜å’Œé£é™©
6. ç»™å‡ºæ€§èƒ½æˆ–å®‰å…¨æ€§æ–¹é¢çš„å»ºè®®

è¯·ç¡®ä¿ä¼˜åŒ–åçš„ä»£ç æ›´åŠ é«˜æ•ˆã€å®‰å…¨ã€å¯ç»´æŠ¤ã€‚"""

        # è°ƒç”¨LLMè¿›è¡Œä¼˜åŒ–
        llm_client = self.llm_manager.get_client()
        initial_output = await llm_client.generate(prompt, model=self.llm_model)
        
        # åº”ç”¨è¾“å‡ºæ§åˆ¶
        control_result = await self.output_controller.control_output(
            original_prompt=prompt,
            agent_output=initial_output,
            output_type="script"
        )
        
        return {
            "success": control_result.success,
            "content": control_result.final_output,
            "original_code": code,
            "optimization_goals": optimization_goals,
            "quality_score": self._calculate_code_quality(control_result),
            "optimization_effectiveness": self._assess_optimization(control_result, code)
        }
    
    async def explain_code(self, code: str, detail_level: str = "intermediate") -> Dict[str, Any]:
        """
        è§£é‡Šä»£ç 
        
        Args:
            code: éœ€è¦è§£é‡Šçš„ä»£ç 
            detail_level: è¯¦ç»†ç¨‹åº¦ (beginner/intermediate/advanced)
            
        Returns:
            åŒ…å«ä»£ç è§£é‡Šçš„å­—å…¸
        """
        level_guide = {
            "beginner": "é¢å‘åˆå­¦è€…ï¼Œè§£é‡ŠåŸºç¡€æ¦‚å¿µå’Œè¯­æ³•",
            "intermediate": "é¢å‘æœ‰ä¸€å®šåŸºç¡€çš„å¼€å‘è€…ï¼Œé‡ç‚¹è¯´æ˜é€»è¾‘å’Œæ€è·¯",
            "advanced": "é¢å‘é«˜çº§å¼€å‘è€…ï¼Œæ·±å…¥åˆ†æç®—æ³•ã€è®¾è®¡æ¨¡å¼å’Œä¼˜åŒ–ç‚¹"
        }
        
        prompt = f"""è¯·è§£é‡Šä»¥ä¸‹ä»£ç ï¼Œè§£é‡Šè¯¦ç»†ç¨‹åº¦ï¼š{level_guide.get(detail_level, 'ä¸­ç­‰')}

ä»£ç ï¼š
```
{code}
```

è§£é‡Šè¦æ±‚ï¼š
1. æ¦‚è¿°ä»£ç çš„ä¸»è¦åŠŸèƒ½å’Œç”¨é€”
2. é€æ­¥è§£é‡Šå…³é”®ä»£ç æ®µçš„ä½œç”¨
3. è¯´æ˜ç®—æ³•é€»è¾‘å’Œæ•°æ®æµç¨‹
4. æŒ‡å‡ºä»£ç ä¸­çš„é‡è¦æŠ€æœ¯ç‚¹
5. åˆ†æä»£ç çš„ä¼˜ç¼ºç‚¹
6. æä¾›æ”¹è¿›å»ºè®®ï¼ˆå¦‚æœ‰ï¼‰

è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è¿›è¡Œè§£é‡Šï¼Œå¸®åŠ©ç†è§£ä»£ç çš„å·¥ä½œåŸç†ã€‚"""

        # è°ƒç”¨LLMè¿›è¡Œè§£é‡Š
        llm_client = self.llm_manager.get_client()
        initial_output = await llm_client.generate(prompt, model=self.llm_model)
        
        # åº”ç”¨è¾“å‡ºæ§åˆ¶ï¼ˆä½¿ç”¨é€šç”¨reportç±»å‹ï¼‰
        control_result = await self.output_controller.control_output(
            original_prompt=prompt,
            agent_output=initial_output,
            output_type="script"
        )
        
        return {
            "success": control_result.success,
            "content": control_result.final_output,
            "original_code": code,
            "detail_level": detail_level,
            "explanation_quality": self._assess_explanation_quality(control_result)
        }
    
    async def review_code(self, code: str, review_focus: List[str] = None) -> Dict[str, Any]:
        """
        ä»£ç å®¡æŸ¥
        
        Args:
            code: éœ€è¦å®¡æŸ¥çš„ä»£ç 
            review_focus: å®¡æŸ¥é‡ç‚¹åˆ—è¡¨
            
        Returns:
            åŒ…å«å®¡æŸ¥ç»“æœçš„å­—å…¸
        """
        if review_focus is None:
            review_focus = ["ä»£ç è´¨é‡", "å®‰å…¨æ€§", "æ€§èƒ½", "å¯ç»´æŠ¤æ€§"]
        
        focus_text = "ã€".join(review_focus)
        
        prompt = f"""è¯·å¯¹ä»¥ä¸‹ä»£ç è¿›è¡Œä¸“ä¸šå®¡æŸ¥ï¼Œé‡ç‚¹å…³æ³¨ï¼š{focus_text}

ä»£ç ï¼š
```
{code}
```

å®¡æŸ¥è¦æ±‚ï¼š
1. ä»£ç è´¨é‡è¯„ä¼°ï¼šè¯­æ³•ã€ç»“æ„ã€å‘½åè§„èŒƒ
2. å®‰å…¨æ€§æ£€æŸ¥ï¼šæ½œåœ¨å®‰å…¨æ¼æ´å’Œé£é™©ç‚¹
3. æ€§èƒ½åˆ†æï¼šæ€§èƒ½ç“¶é¢ˆå’Œä¼˜åŒ–æœºä¼š
4. å¯ç»´æŠ¤æ€§è¯„ä¼°ï¼šä»£ç çš„å¯è¯»æ€§å’Œå¯æ‰©å±•æ€§
5. æœ€ä½³å®è·µæ£€æŸ¥ï¼šæ˜¯å¦éµå¾ªè¡Œä¸šæ ‡å‡†å’Œè§„èŒƒ
6. å…·ä½“æ”¹è¿›å»ºè®®ï¼šæä¾›å¯æ“ä½œçš„ä¼˜åŒ–æ–¹æ¡ˆ

è¯·ç»™å‡ºä¸“ä¸šã€å®¢è§‚çš„å®¡æŸ¥æ„è§å’Œæ”¹è¿›å»ºè®®ã€‚"""

        # è°ƒç”¨LLMè¿›è¡Œå®¡æŸ¥
        llm_client = self.llm_manager.get_client()
        initial_output = await llm_client.generate(prompt, model=self.llm_model)
        
        # åº”ç”¨è¾“å‡ºæ§åˆ¶
        control_result = await self.output_controller.control_output(
            original_prompt=prompt,
            agent_output=initial_output,
            output_type="script"
        )
        
        return {
            "success": control_result.success,
            "content": control_result.final_output,
            "original_code": code,
            "review_focus": review_focus,
            "review_quality": self._assess_review_quality(control_result),
            "security_issues": self._extract_security_issues(control_result)
        }
    
    async def generate_api(self, api_description: str, framework: str = "FastAPI",
                          include_docs: bool = True) -> Dict[str, Any]:
        """
        ç”ŸæˆAPIæ¥å£ä»£ç 
        
        Args:
            api_description: APIåŠŸèƒ½æè¿°
            framework: ä½¿ç”¨çš„æ¡†æ¶
            include_docs: æ˜¯å¦åŒ…å«APIæ–‡æ¡£
            
        Returns:
            åŒ…å«APIä»£ç çš„å­—å…¸
        """
        docs_requirement = "åŒ…å«å®Œæ•´çš„APIæ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹" if include_docs else ""
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹æè¿°ç”Ÿæˆ{framework} APIæ¥å£ä»£ç ï¼š

APIæè¿°ï¼š{api_description}
ä½¿ç”¨æ¡†æ¶ï¼š{framework}

è¦æ±‚ï¼š
1. å®Œæ•´çš„APIè·¯ç”±å®šä¹‰
2. è¯·æ±‚å’Œå“åº”æ¨¡å‹å®šä¹‰
3. é€‚å½“çš„HTTPçŠ¶æ€ç å¤„ç†
4. è¾“å…¥éªŒè¯å’Œé”™è¯¯å¤„ç†
5. å®‰å…¨æ€§è€ƒè™‘ï¼ˆè®¤è¯ã€æˆæƒç­‰ï¼‰
6. ä»£ç æ³¨é‡Šå’Œç±»å‹æ³¨è§£
{f'7. {docs_requirement}' if include_docs else ''}

è¯·ç¡®ä¿APIè®¾è®¡ç¬¦åˆRESTfulè§„èŒƒï¼Œä»£ç å®‰å…¨å¯é ã€‚"""

        # è°ƒç”¨LLMç”ŸæˆAPIä»£ç 
        llm_client = self.llm_manager.get_client()
        initial_output = await llm_client.generate(prompt, model=self.llm_model)
        
        # åº”ç”¨è¾“å‡ºæ§åˆ¶
        control_result = await self.output_controller.control_output(
            original_prompt=prompt,
            agent_output=initial_output,
            output_type="api"
        )
        
        return {
            "success": control_result.success,
            "content": control_result.final_output,
            "api_description": api_description,
            "framework": framework,
            "include_docs": include_docs,
            "quality_score": self._calculate_code_quality(control_result),
            "api_design_score": self._assess_api_design(control_result)
        }
    
    def _calculate_code_quality(self, control_result) -> float:
        """è®¡ç®—ä»£ç è´¨é‡åˆ†æ•°"""
        if not control_result.success:
            return 0.0
        
        # åŸºç¡€åˆ†æ•°
        base_score = 8.0
        
        # æ£€æŸ¥å®‰å…¨æ€§çº¦æŸ
        security_violations = [r for r in control_result.constraint_violations 
                             if 'security' in r.rule_name.lower() and r.status.value == 'failed']
        if security_violations:
            base_score -= 2.0  # å®‰å…¨é—®é¢˜ä¸¥é‡æ‰£åˆ†
        
        # æ£€æŸ¥AIä»£ç å®¡æŸ¥ç»“æœ
        ai_results = [r for r in control_result.validation_results 
                     if r.rule_name == 'ai_self_check' and r.details]
        if ai_results:
            ai_details = ai_results[0].details.get('ai_check_results', [])
            for check in ai_details:
                if check.get('type') == 'code_quality':
                    syntax_score = check.get('syntax', 0)
                    security_score = check.get('security', 0)
                    readability_score = check.get('readability', 0)
                    # åŠ æƒå¹³å‡
                    weighted_score = (syntax_score * 0.3 + security_score * 0.4 + readability_score * 0.3) / 10 * 2
                    base_score = max(base_score, base_score + weighted_score - 1.0)
        
        return max(0.0, min(10.0, base_score))
    
    def _assess_security(self, control_result) -> float:
        """è¯„ä¼°ä»£ç å®‰å…¨æ€§"""
        if not control_result.success:
            return 0.0
        
        # æ£€æŸ¥å®‰å…¨ç›¸å…³çš„çº¦æŸè¿å
        security_violations = [r for r in control_result.constraint_violations 
                             if 'security' in r.rule_name.lower() and r.status.value == 'failed']
        
        # åŸºç¡€å®‰å…¨åˆ†æ•°
        base_score = 10.0
        
        # æ¯ä¸ªå®‰å…¨è¿åæ‰£åˆ†
        base_score -= len(security_violations) * 2.0
        
        # æ£€æŸ¥AIå®‰å…¨æ£€æŸ¥ç»“æœ
        ai_results = [r for r in control_result.validation_results 
                     if r.rule_name == 'ai_self_check' and r.details]
        if ai_results:
            ai_details = ai_results[0].details.get('ai_check_results', [])
            for check in ai_details:
                if check.get('type') == 'security_check' and not check.get('secure', True):
                    base_score -= 1.0
        
        return max(0.0, min(10.0, base_score))
    
    def _assess_design_quality(self, control_result) -> str:
        """è¯„ä¼°è®¾è®¡è´¨é‡"""
        quality_score = self._calculate_code_quality(control_result)
        if quality_score >= 8.5:
            return "ä¼˜ç§€"
        elif quality_score >= 7.0:
            return "è‰¯å¥½"
        elif quality_score >= 6.0:
            return "ä¸€èˆ¬"
        else:
            return "éœ€æ”¹è¿›"
    
    def _assess_optimization(self, control_result, original_code: str) -> str:
        """è¯„ä¼°ä¼˜åŒ–æ•ˆæœ"""
        if not control_result.success:
            return "å¤±è´¥"
        
        # ç®€å•çš„ä¼˜åŒ–æ•ˆæœè¯„ä¼°
        final_code = control_result.final_output
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¼˜åŒ–è¯´æ˜
        if "ä¼˜åŒ–" in final_code and "æ”¹è¿›" in final_code:
            return "æ˜¾è‘—"
        elif "æ”¹è¿›" in final_code or "ä¼˜åŒ–" in final_code:
            return "ä¸­ç­‰"
        else:
            return "è½»å¾®"
    
    def _assess_explanation_quality(self, control_result) -> str:
        """è¯„ä¼°è§£é‡Šè´¨é‡"""
        if not control_result.success:
            return "å·®"
        
        content = control_result.final_output.lower()
        quality_indicators = ["åŠŸèƒ½", "é€»è¾‘", "ç®—æ³•", "åŸç†", "ä½œç”¨"]
        found_indicators = sum(1 for indicator in quality_indicators if indicator in content)
        
        ratio = found_indicators / len(quality_indicators)
        if ratio >= 0.8:
            return "ä¼˜ç§€"
        elif ratio >= 0.6:
            return "è‰¯å¥½"
        elif ratio >= 0.4:
            return "ä¸€èˆ¬"
        else:
            return "å·®"
    
    def _assess_review_quality(self, control_result) -> str:
        """è¯„ä¼°å®¡æŸ¥è´¨é‡"""
        if not control_result.success:
            return "å·®"
        
        content = control_result.final_output.lower()
        review_aspects = ["è´¨é‡", "å®‰å…¨", "æ€§èƒ½", "ç»´æŠ¤", "å»ºè®®"]
        found_aspects = sum(1 for aspect in review_aspects if aspect in content)
        
        ratio = found_aspects / len(review_aspects)
        if ratio >= 0.8:
            return "å…¨é¢"
        elif ratio >= 0.6:
            return "è¾ƒå¥½"
        else:
            return "ç®€å•"
    
    def _extract_security_issues(self, control_result) -> List[str]:
        """æå–å®‰å…¨é—®é¢˜"""
        issues = []
        
        # ä»çº¦æŸè¿åä¸­æå–å®‰å…¨é—®é¢˜
        security_violations = [r for r in control_result.constraint_violations 
                             if 'security' in r.rule_name.lower() and r.status.value == 'failed']
        for violation in security_violations:
            issues.append(violation.message)
        
        return issues
    
    def _assess_api_design(self, control_result) -> str:
        """è¯„ä¼°APIè®¾è®¡è´¨é‡"""
        if not control_result.success:
            return "å·®"
        
        content = control_result.final_output.lower()
        api_elements = ["è·¯ç”±", "æ¨¡å‹", "éªŒè¯", "é”™è¯¯", "æ–‡æ¡£"]
        found_elements = sum(1 for element in api_elements if element in content)
        
        ratio = found_elements / len(api_elements)
        if ratio >= 0.8:
            return "ä¼˜ç§€"
        elif ratio >= 0.6:
            return "è‰¯å¥½"
        else:
            return "åŸºç¡€"
    
    def _get_processing_info(self, control_result) -> Dict[str, Any]:
        """è·å–å¤„ç†ä¿¡æ¯æ‘˜è¦"""
        return {
            "constraint_violations": len([r for r in control_result.constraint_violations if r.status.value == 'failed']),
            "validation_results": len([r for r in control_result.validation_results if r.status.value == 'passed']),
            "processing_results": len([r for r in control_result.processing_results if r.changes_made]),
            "re_prompt_attempts": control_result.re_prompt_attempts,
            "processing_time": control_result.processing_time
        }


async def demo_code_assistant():
    """ä»£ç åŠ©æ‰‹æ¼”ç¤º"""
    print("ğŸ’» ä»£ç åŠ©æ‰‹AgentWorkeræ¼”ç¤º")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–å·¥ä½œå™¨
        worker = CodeAssistantWorker()
        
        # æ¼”ç¤º1ï¼šç”Ÿæˆå‡½æ•°
        print("\nğŸ”§ æ¼”ç¤º1ï¼šç”Ÿæˆå‡½æ•°")
        function_result = await worker.generate_function(
            function_description="å®ç°ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ç¬¬né¡¹çš„å‡½æ•°ï¼Œæ”¯æŒç¼“å­˜ä¼˜åŒ–",
            language="python",
            include_tests=True
        )
        
        print(f"âœ… å‡½æ•°ç”Ÿæˆ{'æˆåŠŸ' if function_result['success'] else 'å¤±è´¥'}")
        print(f"ğŸ† è´¨é‡è¯„åˆ†: {function_result['quality_score']:.1f}/10")
        print(f"ğŸ”’ å®‰å…¨è¯„åˆ†: {function_result['security_score']:.1f}/10")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {function_result['processing_time']:.2f}s")
        
        # æ¼”ç¤º2ï¼šç”Ÿæˆç±»
        print("\nğŸ”§ æ¼”ç¤º2ï¼šç”Ÿæˆç±»")
        class_result = await worker.generate_class(
            class_description="è®¾è®¡ä¸€ä¸ªç®€å•çš„é“¶è¡Œè´¦æˆ·ç±»ï¼Œæ”¯æŒå­˜æ¬¾ã€å–æ¬¾ã€æŸ¥è¯¢ä½™é¢ç­‰æ“ä½œ",
            language="python",
            include_examples=True
        )
        
        print(f"âœ… ç±»ç”Ÿæˆ{'æˆåŠŸ' if class_result['success'] else 'å¤±è´¥'}")
        print(f"ğŸ† è´¨é‡è¯„åˆ†: {class_result['quality_score']:.1f}/10")
        print(f"ğŸ¨ è®¾è®¡è´¨é‡: {class_result['design_quality']}")
        
        # æ¼”ç¤º3ï¼šä»£ç ä¼˜åŒ–
        print("\nğŸ”§ æ¼”ç¤º3ï¼šä»£ç ä¼˜åŒ–")
        sample_code = """
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):
        for j in range(0, n-i-1):
            if arr[j] > arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]
    return arr
"""
        
        optimize_result = await worker.optimize_code(
            code=sample_code,
            optimization_goals=["æ€§èƒ½", "å¯è¯»æ€§"]
        )
        
        print(f"âœ… ä»£ç ä¼˜åŒ–{'æˆåŠŸ' if optimize_result['success'] else 'å¤±è´¥'}")
        print(f"ğŸ† è´¨é‡è¯„åˆ†: {optimize_result['quality_score']:.1f}/10")
        print(f"âš¡ ä¼˜åŒ–æ•ˆæœ: {optimize_result['optimization_effectiveness']}")
        
        # æ¼”ç¤º4ï¼šä»£ç è§£é‡Š
        print("\nğŸ”§ æ¼”ç¤º4ï¼šä»£ç è§£é‡Š")
        explain_result = await worker.explain_code(
            code=sample_code,
            detail_level="intermediate"
        )
        
        print(f"âœ… ä»£ç è§£é‡Š{'æˆåŠŸ' if explain_result['success'] else 'å¤±è´¥'}")
        print(f"ğŸ“– è§£é‡Šè´¨é‡: {explain_result['explanation_quality']}")
        
        # æ¼”ç¤º5ï¼šä»£ç å®¡æŸ¥
        print("\nğŸ”§ æ¼”ç¤º5ï¼šä»£ç å®¡æŸ¥")
        review_result = await worker.review_code(
            code=sample_code,
            review_focus=["ä»£ç è´¨é‡", "æ€§èƒ½", "å¯ç»´æŠ¤æ€§"]
        )
        
        print(f"âœ… ä»£ç å®¡æŸ¥{'æˆåŠŸ' if review_result['success'] else 'å¤±è´¥'}")
        print(f"ğŸ“‹ å®¡æŸ¥è´¨é‡: {review_result['review_quality']}")
        if review_result['security_issues']:
            print(f"âš ï¸ å®‰å…¨é—®é¢˜: {len(review_result['security_issues'])}ä¸ª")
        
        print("\nğŸ‰ ä»£ç åŠ©æ‰‹æ¼”ç¤ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(demo_code_assistant())