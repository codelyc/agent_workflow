"""
æ•°æ®åˆ†æAgentWorker

ä¸“é—¨ç”¨äºæ•°æ®æŠ¥å‘Šã€è¶‹åŠ¿åˆ†æã€å¯¹æ¯”åˆ†æç­‰æ•°æ®å¤„ç†ä»»åŠ¡çš„Agentå·¥ä½œæµ
"""

import asyncio
import sys
from pathlib import Path
from typing import Dict, Any, Optional, List
import json

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.append(str(PROJECT_ROOT / "src"))

from core.output_control import OutputController
from core.config.llm_manager import LLMManager


class DataAnalysisWorker:
    """æ•°æ®åˆ†æå·¥ä½œå™¨"""
    
    def __init__(self, llm_model: str = "gpt-4o"):
        """
        åˆå§‹åŒ–æ•°æ®åˆ†æå·¥ä½œå™¨
        
        Args:
            llm_model: ä½¿ç”¨çš„LLMæ¨¡å‹ï¼Œæ•°æ®åˆ†ææ¨èä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹
        """
        self.llm_model = llm_model
        self.llm_manager = LLMManager()
        
        # ä»æœ¬åœ°é…ç½®åŠ è½½è¾“å‡ºæ§åˆ¶å™¨
        config_path = Path(__file__).parent / "configs" / "output_control.yaml"
        self.output_controller = OutputController.from_config_file(
            str(config_path),
            llm_client=self.llm_manager.get_client()
        )
        
        print(f"âœ… æ•°æ®åˆ†æå·¥ä½œå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {llm_model}")
    
    async def generate_data_report(self, data_description: str, analysis_focus: str, report_type: str = "comprehensive") -> Dict[str, Any]:
        """
        ç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Š
        
        Args:
            data_description: æ•°æ®æè¿°
            analysis_focus: åˆ†æé‡ç‚¹
            report_type: æŠ¥å‘Šç±»å‹ (comprehensive/summary/detailed)
            
        Returns:
            åŒ…å«åˆ†ææŠ¥å‘Šå’Œè´¨é‡ä¿¡æ¯çš„å­—å…¸
        """
        report_templates = {
            "comprehensive": "åŒ…å«æ‰§è¡Œæ‘˜è¦ã€æ•°æ®æ¦‚è¿°ã€è¯¦ç»†åˆ†æã€ç»“è®ºå»ºè®®çš„å®Œæ•´æŠ¥å‘Š",
            "summary": "é‡ç‚¹çªå‡ºå…³é”®å‘ç°å’Œä¸»è¦ç»“è®ºçš„ç®€è¦æŠ¥å‘Š",
            "detailed": "æ·±å…¥åˆ†æå„ä¸ªç»´åº¦ï¼Œæä¾›è¯¦ç»†æ•°æ®æ”¯æ’‘çš„ä¸“ä¸šæŠ¥å‘Š"
        }
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹æ•°æ®ä¿¡æ¯ç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Šï¼š

æ•°æ®æè¿°ï¼š{data_description}
åˆ†æé‡ç‚¹ï¼š{analysis_focus}
æŠ¥å‘Šç±»å‹ï¼š{report_templates.get(report_type, 'ç»¼åˆæŠ¥å‘Š')}

æŠ¥å‘Šè¦æ±‚ï¼š
1. æ•°æ®æ¦‚è¿°ï¼šç®€è¦æè¿°æ•°æ®çš„åŸºæœ¬æƒ…å†µ
2. å…³é”®å‘ç°ï¼šè¯†åˆ«æ•°æ®ä¸­çš„é‡è¦æ¨¡å¼ã€è¶‹åŠ¿å’Œå¼‚å¸¸
3. åˆ†æç»“è®ºï¼šåŸºäºæ•°æ®å¾—å‡ºå®¢è§‚ã€å‡†ç¡®çš„ç»“è®º
4. è¡ŒåŠ¨å»ºè®®ï¼šæä¾›åŸºäºåˆ†æç»“æœçš„å®ç”¨å»ºè®®
5. ä½¿ç”¨ç»“æ„åŒ–æ ¼å¼ï¼ˆMarkdownï¼‰
6. ç¡®ä¿åˆ†æé€»è¾‘æ¸…æ™°ï¼Œç»“è®ºæœ‰æ•°æ®æ”¯æ’‘

è¯·ä¿æŒå®¢è§‚æ€§ï¼Œé¿å…ä¸»è§‚è‡†æµ‹ï¼Œæ‰€æœ‰ç»“è®ºéƒ½è¦æœ‰æ•°æ®ä¾æ®ã€‚"""

        # è°ƒç”¨LLMç”Ÿæˆåˆå§‹åˆ†æ
        llm_client = self.llm_manager.get_client()
        initial_output = await llm_client.generate(prompt, model=self.llm_model)
        
        # åº”ç”¨è¾“å‡ºæ§åˆ¶
        control_result = await self.output_controller.control_output(
            original_prompt=prompt,
            agent_output=initial_output,
            output_type="report"
        )
        
        return {
            "success": control_result.success,
            "content": control_result.final_output,
            "data_description": data_description,
            "analysis_focus": analysis_focus,
            "report_type": report_type,
            "quality_score": self._calculate_analysis_quality(control_result),
            "logical_consistency": self._check_logical_consistency(control_result),
            "processing_time": control_result.processing_time,
            "processing_info": self._get_processing_info(control_result)
        }
    
    async def perform_trend_analysis(self, time_series_data: str, time_period: str, prediction_horizon: str = "3ä¸ªæœˆ") -> Dict[str, Any]:
        """
        æ‰§è¡Œè¶‹åŠ¿åˆ†æ
        
        Args:
            time_series_data: æ—¶é—´åºåˆ—æ•°æ®æè¿°
            time_period: åˆ†ææ—¶é—´æ®µ
            prediction_horizon: é¢„æµ‹æ—¶é—´èŒƒå›´
            
        Returns:
            åŒ…å«è¶‹åŠ¿åˆ†æç»“æœçš„å­—å…¸
        """
        prompt = f"""è¯·å¯¹ä»¥ä¸‹æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œè¶‹åŠ¿åˆ†æï¼š

æ•°æ®ä¿¡æ¯ï¼š{time_series_data}
åˆ†ææ—¶é—´æ®µï¼š{time_period}
é¢„æµ‹æ—¶é—´èŒƒå›´ï¼š{prediction_horizon}

åˆ†æè¦æ±‚ï¼š
1. å†å²è¶‹åŠ¿åˆ†æï¼šè¯†åˆ«æ•°æ®çš„å†å²å‘å±•æ¨¡å¼
2. å½“å‰çŠ¶æ€è¯„ä¼°ï¼šåˆ†æå½“å‰æ•°æ®çŠ¶æ€å’Œç‰¹å¾
3. æœªæ¥è¶‹åŠ¿é¢„æµ‹ï¼šåŸºäºå†å²æ¨¡å¼é¢„æµ‹æœªæ¥å‘å±•è¶‹åŠ¿
4. å½±å“å› ç´ åˆ†æï¼šè¯†åˆ«å¯èƒ½å½±å“è¶‹åŠ¿çš„å…³é”®å› ç´ 
5. é£é™©æç¤ºï¼šæŒ‡å‡ºè¶‹åŠ¿åˆ†æä¸­çš„ä¸ç¡®å®šæ€§å’Œé£é™©

è¯·ç¡®ä¿åˆ†æåŸºäºæ•°æ®è§„å¾‹ï¼Œé¢„æµ‹è¦åˆç†è°¨æ…ã€‚"""

        # è°ƒç”¨LLMç”Ÿæˆåˆ†æ
        llm_client = self.llm_manager.get_client()
        initial_output = await llm_client.generate(prompt, model=self.llm_model)
        
        # åº”ç”¨è¾“å‡ºæ§åˆ¶
        control_result = await self.output_controller.control_output(
            original_prompt=prompt,
            agent_output=initial_output,
            output_type="trend_analysis"
        )
        
        return {
            "success": control_result.success,
            "content": control_result.final_output,
            "time_series_data": time_series_data,
            "time_period": time_period,
            "prediction_horizon": prediction_horizon,
            "quality_score": self._calculate_analysis_quality(control_result),
            "trend_reliability": self._assess_trend_reliability(control_result)
        }
    
    async def perform_comparison_analysis(self, comparison_data: List[Dict], comparison_dimensions: List[str]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå¯¹æ¯”åˆ†æ
        
        Args:
            comparison_data: å¯¹æ¯”æ•°æ®åˆ—è¡¨
            comparison_dimensions: å¯¹æ¯”ç»´åº¦åˆ—è¡¨
            
        Returns:
            åŒ…å«å¯¹æ¯”åˆ†æç»“æœçš„å­—å…¸
        """
        data_summary = "\n".join([f"æ•°æ®ç»„{i+1}: {data}" for i, data in enumerate(comparison_data)])
        dimensions_text = "ã€".join(comparison_dimensions)
        
        prompt = f"""è¯·å¯¹ä»¥ä¸‹æ•°æ®è¿›è¡Œå¤šç»´åº¦å¯¹æ¯”åˆ†æï¼š

å¯¹æ¯”æ•°æ®ï¼š
{data_summary}

å¯¹æ¯”ç»´åº¦ï¼š{dimensions_text}

åˆ†æè¦æ±‚ï¼š
1. å¯¹æ¯”ç»´åº¦åˆ†æï¼šé€ä¸€åˆ†æå„ä¸ªç»´åº¦çš„å·®å¼‚
2. å·®å¼‚åŸå› æ¢è®¨ï¼šåˆ†æé€ æˆå·®å¼‚çš„å¯èƒ½åŸå› 
3. ä¼˜åŠ£åŠ¿è¯„ä¼°ï¼šå®¢è§‚è¯„ä¼°å„æ•°æ®ç»„çš„ä¼˜åŠ£åŠ¿
4. å…³é”®æ´å¯Ÿï¼šæç‚¼å¯¹æ¯”åˆ†æä¸­çš„å…³é”®å‘ç°
5. å†³ç­–å»ºè®®ï¼šåŸºäºå¯¹æ¯”ç»“æœæä¾›å†³ç­–å‚è€ƒ

è¯·ä¿æŒå®¢è§‚ä¸­ç«‹ï¼ŒåŸºäºæ•°æ®è¿›è¡Œåˆ†æã€‚"""

        # è°ƒç”¨LLMç”Ÿæˆåˆ†æ
        llm_client = self.llm_manager.get_client()
        initial_output = await llm_client.generate(prompt, model=self.llm_model)
        
        # åº”ç”¨è¾“å‡ºæ§åˆ¶
        control_result = await self.output_controller.control_output(
            original_prompt=prompt,
            agent_output=initial_output,
            output_type="comparison"
        )
        
        return {
            "success": control_result.success,
            "content": control_result.final_output,
            "comparison_data": comparison_data,
            "comparison_dimensions": comparison_dimensions,
            "quality_score": self._calculate_analysis_quality(control_result),
            "comparison_completeness": self._check_comparison_completeness(control_result)
        }
    
    async def generate_insights_summary(self, raw_data: str, business_context: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ•°æ®æ´å¯Ÿæ‘˜è¦
        
        Args:
            raw_data: åŸå§‹æ•°æ®æè¿°
            business_context: ä¸šåŠ¡ä¸Šä¸‹æ–‡
            
        Returns:
            åŒ…å«æ´å¯Ÿæ‘˜è¦çš„å­—å…¸
        """
        prompt = f"""è¯·åŸºäºä»¥ä¸‹åŸå§‹æ•°æ®ç”Ÿæˆä¸šåŠ¡æ´å¯Ÿæ‘˜è¦ï¼š

åŸå§‹æ•°æ®ï¼š{raw_data}
ä¸šåŠ¡èƒŒæ™¯ï¼š{business_context}

æ´å¯Ÿè¦æ±‚ï¼š
1. å…³é”®æ•°æ®å‘ç°ï¼šæç‚¼æœ€é‡è¦çš„æ•°æ®å‘ç°
2. ä¸šåŠ¡å½±å“åˆ†æï¼šåˆ†ææ•°æ®å¯¹ä¸šåŠ¡çš„æ½œåœ¨å½±å“
3. æœºä¼šè¯†åˆ«ï¼šè¯†åˆ«æ•°æ®ä¸­æ˜¾ç¤ºçš„ä¸šåŠ¡æœºä¼š
4. é£é™©è­¦ç¤ºï¼šæŒ‡å‡ºæ•°æ®ä¸­åæ˜ çš„æ½œåœ¨é£é™©
5. è¡ŒåŠ¨å»ºè®®ï¼šæä¾›å…·ä½“å¯è¡Œçš„è¡ŒåŠ¨å»ºè®®

è¯·ç¡®ä¿æ´å¯Ÿå…·æœ‰å®é™…ä¸šåŠ¡ä»·å€¼ï¼Œå»ºè®®å¯æ“ä½œæ€§å¼ºã€‚"""

        # è°ƒç”¨LLMç”Ÿæˆæ´å¯Ÿ
        llm_client = self.llm_manager.get_client()
        initial_output = await llm_client.generate(prompt, model=self.llm_model)
        
        # åº”ç”¨è¾“å‡ºæ§åˆ¶ï¼ˆä½¿ç”¨é€šç”¨reportç±»å‹ï¼‰
        control_result = await self.output_controller.control_output(
            original_prompt=prompt,
            agent_output=initial_output,
            output_type="report"
        )
        
        return {
            "success": control_result.success,
            "content": control_result.final_output,
            "raw_data": raw_data,
            "business_context": business_context,
            "quality_score": self._calculate_analysis_quality(control_result),
            "insight_value": self._assess_insight_value(control_result)
        }
    
    def _calculate_analysis_quality(self, control_result) -> float:
        """è®¡ç®—åˆ†æè´¨é‡åˆ†æ•°"""
        if not control_result.success:
            return 0.0
        
        # åŸºç¡€åˆ†æ•°
        base_score = 8.0  # æ•°æ®åˆ†æè¦æ±‚æ›´é«˜çš„åŸºç¡€è´¨é‡
        
        # æ£€æŸ¥çº¦æŸè¿åæƒ…å†µ
        critical_violations = [r for r in control_result.constraint_violations 
                             if r.status.value == 'failed' and 'accuracy' in r.rule_name.lower()]
        if not critical_violations:
            base_score += 1.0
        
        # æ£€æŸ¥AIè‡ªæ£€ç»“æœ
        ai_results = [r for r in control_result.validation_results 
                     if r.rule_name == 'ai_self_check' and r.details]
        if ai_results:
            ai_details = ai_results[0].details.get('ai_check_results', [])
            for check in ai_details:
                if check.get('type') == 'analysis_quality':
                    logic_score = check.get('logic', 0)
                    accuracy_score = check.get('accuracy', 0)
                    # é€»è¾‘æ€§å’Œå‡†ç¡®æ€§æƒé‡æ›´é«˜
                    weighted_score = (logic_score * 0.4 + accuracy_score * 0.4) / 10 * 2
                    base_score = max(base_score, base_score + weighted_score - 1.0)
        
        # re-promptæ¬¡æ•°å½±å“
        if control_result.re_prompt_attempts > 0:
            base_score -= 0.3 * control_result.re_prompt_attempts
        
        return max(0.0, min(10.0, base_score))
    
    def _check_logical_consistency(self, control_result) -> bool:
        """æ£€æŸ¥é€»è¾‘ä¸€è‡´æ€§"""
        # æ£€æŸ¥æ˜¯å¦æœ‰é€»è¾‘çŸ›ç›¾ç›¸å…³çš„éªŒè¯å¤±è´¥
        logic_failures = [r for r in control_result.validation_results 
                         if 'consistency' in r.rule_name.lower() and r.status.value == 'failed']
        return len(logic_failures) == 0
    
    def _assess_trend_reliability(self, control_result) -> str:
        """è¯„ä¼°è¶‹åŠ¿åˆ†æå¯é æ€§"""
        if not control_result.success:
            return "ä½"
        
        # åŸºäºè´¨é‡åˆ†æ•°è¯„ä¼°å¯é æ€§
        quality = self._calculate_analysis_quality(control_result)
        if quality >= 8.5:
            return "é«˜"
        elif quality >= 7.0:
            return "ä¸­"
        else:
            return "ä½"
    
    def _check_comparison_completeness(self, control_result) -> float:
        """æ£€æŸ¥å¯¹æ¯”åˆ†æå®Œæ•´æ€§"""
        if not control_result.success:
            return 0.0
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„å¯¹æ¯”è¦ç´ 
        content = control_result.final_output.lower()
        required_elements = ['å¯¹æ¯”', 'å·®å¼‚', 'ä¼˜åŠ¿', 'åŠ£åŠ¿', 'å»ºè®®']
        found_elements = sum(1 for element in required_elements if element in content)
        
        return found_elements / len(required_elements)
    
    def _assess_insight_value(self, control_result) -> str:
        """è¯„ä¼°æ´å¯Ÿä»·å€¼"""
        if not control_result.success:
            return "ä½"
        
        content = control_result.final_output.lower()
        value_indicators = ['æœºä¼š', 'é£é™©', 'å»ºè®®', 'è¡ŒåŠ¨', 'å½±å“', 'å‘ç°']
        found_indicators = sum(1 for indicator in value_indicators if indicator in content)
        
        value_ratio = found_indicators / len(value_indicators)
        if value_ratio >= 0.7:
            return "é«˜"
        elif value_ratio >= 0.5:
            return "ä¸­"
        else:
            return "ä½"
    
    def _get_processing_info(self, control_result) -> Dict[str, Any]:
        """è·å–å¤„ç†ä¿¡æ¯æ‘˜è¦"""
        return {
            "constraint_violations": len([r for r in control_result.constraint_violations if r.status.value == 'failed']),
            "validation_results": len([r for r in control_result.validation_results if r.status.value == 'passed']),
            "processing_results": len([r for r in control_result.processing_results if r.changes_made]),
            "re_prompt_attempts": control_result.re_prompt_attempts,
            "processing_time": control_result.processing_time
        }
    
    async def batch_analyze(self, analysis_tasks: List[Dict]) -> Dict[str, Any]:
        """æ‰¹é‡æ•°æ®åˆ†æ"""
        results = []
        
        for task in analysis_tasks:
            task_type = task.get('type', 'report')
            
            try:
                if task_type == 'report':
                    result = await self.generate_data_report(
                        data_description=task['data_description'],
                        analysis_focus=task['analysis_focus'],
                        report_type=task.get('report_type', 'comprehensive')
                    )
                elif task_type == 'trend':
                    result = await self.perform_trend_analysis(
                        time_series_data=task['time_series_data'],
                        time_period=task['time_period'],
                        prediction_horizon=task.get('prediction_horizon', '3ä¸ªæœˆ')
                    )
                elif task_type == 'comparison':
                    result = await self.perform_comparison_analysis(
                        comparison_data=task['comparison_data'],
                        comparison_dimensions=task['comparison_dimensions']
                    )
                elif task_type == 'insights':
                    result = await self.generate_insights_summary(
                        raw_data=task['raw_data'],
                        business_context=task['business_context']
                    )
                else:
                    result = {"success": False, "error": f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {task_type}"}
                
                results.append(result)
                
            except Exception as e:
                results.append({"success": False, "error": str(e)})
        
        return {
            "total_tasks": len(analysis_tasks),
            "successful_tasks": sum(1 for r in results if r.get('success', False)),
            "average_quality": sum(r.get('quality_score', 0) for r in results) / len(results) if results else 0,
            "results": results
        }


async def demo_data_analysis():
    """æ•°æ®åˆ†ææ¼”ç¤º"""
    print("ğŸ“Š æ•°æ®åˆ†æAgentWorkeræ¼”ç¤º")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–å·¥ä½œå™¨
        worker = DataAnalysisWorker()
        
        # æ¼”ç¤º1ï¼šæ•°æ®æŠ¥å‘Šç”Ÿæˆ
        print("\nğŸ“ˆ æ¼”ç¤º1ï¼šç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Š")
        report_result = await worker.generate_data_report(
            data_description="2024å¹´Q1-Q3å­£åº¦é”€å”®æ•°æ®ï¼ŒåŒ…å«äº§å“ç±»åˆ«ã€åœ°åŒºåˆ†å¸ƒã€å®¢æˆ·ç¾¤ä½“ç­‰ç»´åº¦",
            analysis_focus="å­£åº¦é”€å”®è¶‹åŠ¿å’Œäº§å“è¡¨ç°åˆ†æ",
            report_type="comprehensive"
        )
        
        print(f"âœ… æŠ¥å‘Šç”Ÿæˆ{'æˆåŠŸ' if report_result['success'] else 'å¤±è´¥'}")
        print(f"ğŸ† è´¨é‡è¯„åˆ†: {report_result['quality_score']:.1f}/10")
        print(f"ğŸ§  é€»è¾‘ä¸€è‡´æ€§: {'æ˜¯' if report_result['logical_consistency'] else 'å¦'}")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {report_result['processing_time']:.2f}s")
        
        # æ¼”ç¤º2ï¼šè¶‹åŠ¿åˆ†æ
        print("\nğŸ“ˆ æ¼”ç¤º2ï¼šæ‰§è¡Œè¶‹åŠ¿åˆ†æ")
        trend_result = await worker.perform_trend_analysis(
            time_series_data="è¿‡å»12ä¸ªæœˆçš„ç½‘ç«™è®¿é—®é‡æ•°æ®ï¼ŒåŒ…å«æœˆåº¦è®¿é—®é‡ã€ç”¨æˆ·æ¥æºã€é¡µé¢åœç•™æ—¶é—´ç­‰æŒ‡æ ‡",
            time_period="2024å¹´1æœˆ-12æœˆ",
            prediction_horizon="æœªæ¥6ä¸ªæœˆ"
        )
        
        print(f"âœ… è¶‹åŠ¿åˆ†æ{'æˆåŠŸ' if trend_result['success'] else 'å¤±è´¥'}")
        print(f"ğŸ† è´¨é‡è¯„åˆ†: {trend_result['quality_score']:.1f}/10")
        print(f"ğŸ¯ è¶‹åŠ¿å¯é æ€§: {trend_result['trend_reliability']}")
        
        # æ¼”ç¤º3ï¼šå¯¹æ¯”åˆ†æ
        print("\nğŸ“ˆ æ¼”ç¤º3ï¼šæ‰§è¡Œå¯¹æ¯”åˆ†æ")
        comparison_result = await worker.perform_comparison_analysis(
            comparison_data=[
                {"name": "äº§å“A", "sales": "100ä¸‡", "growth": "15%", "satisfaction": "85%"},
                {"name": "äº§å“B", "sales": "80ä¸‡", "growth": "25%", "satisfaction": "90%"},
                {"name": "äº§å“C", "sales": "120ä¸‡", "growth": "5%", "satisfaction": "78%"}
            ],
            comparison_dimensions=["é”€å”®é¢", "å¢é•¿ç‡", "å®¢æˆ·æ»¡æ„åº¦", "å¸‚åœºæ½œåŠ›"]
        )
        
        print(f"âœ… å¯¹æ¯”åˆ†æ{'æˆåŠŸ' if comparison_result['success'] else 'å¤±è´¥'}")
        print(f"ğŸ† è´¨é‡è¯„åˆ†: {comparison_result['quality_score']:.1f}/10")
        print(f"ğŸ“‹ åˆ†æå®Œæ•´æ€§: {comparison_result['comparison_completeness']:.1%}")
        
        # æ¼”ç¤º4ï¼šæ´å¯Ÿæ‘˜è¦
        print("\nğŸ“ˆ æ¼”ç¤º4ï¼šç”Ÿæˆæ´å¯Ÿæ‘˜è¦")
        insights_result = await worker.generate_insights_summary(
            raw_data="ç”¨æˆ·è¡Œä¸ºæ•°æ®æ˜¾ç¤ºï¼šç§»åŠ¨ç«¯è®¿é—®å 70%ï¼Œè´­ä¹°è½¬åŒ–ç‡ä¸º3.2%ï¼Œå¹³å‡è®¢å•é‡‘é¢ä¸º268å…ƒ",
            business_context="ç”µå•†å¹³å°å¸Œæœ›æå‡ç”¨æˆ·ä½“éªŒå’Œé”€å”®è½¬åŒ–"
        )
        
        print(f"âœ… æ´å¯Ÿç”Ÿæˆ{'æˆåŠŸ' if insights_result['success'] else 'å¤±è´¥'}")
        print(f"ğŸ† è´¨é‡è¯„åˆ†: {insights_result['quality_score']:.1f}/10")
        print(f"ğŸ’ æ´å¯Ÿä»·å€¼: {insights_result['insight_value']}")
        
        print("\nğŸ‰ æ•°æ®åˆ†ææ¼”ç¤ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(demo_data_analysis())